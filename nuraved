algorith  = way to learn by m/c

algo+ data = ai/ml model

ML 1950== ML
PREDICTION  === 500 FEET SPACE WHAT PRICE BY LINE ON CHART
CLASSIFICATION = HE IS PASS OR NOT   SAME E LINE HIKADCHE PASS LINECHYA AND TIKADCHE FAIL

200= DEEP LEARN
FIND DOG , CAT
- BUT NO MEMORY

2010= RNN , LSTM
HAVE SMALL MEMORY ONLY

2017 -- PAPER BY GOOGLE "ATTENTION ALL U NEED"
THEY GAVE TRANSFORER MODEL = HAVE LARGE MEMORY ALSO WHICH NOT THERE IN RNN + LSTM
-TRANSFORMER == 
1)ENCODING  - CONVERT WORD INTO NUMBER --GOOGLE WORK ON IT AND DEVELOP= BERT
2)DECODING = GENERATE NEW WORDS - OPENAI OF ALTMAN GAVE = 2017=GPT-117 MILLION PARA  

2022= ANTHROPIC --- CLAODE MODEL 4.1 CURRENT
      GOOGLE   ---- BARD , GEMINI MODEL
META --- LLAMA
MISTRAL -- MISTRAL
2025- DEEPSEEK

-SO APROX 1 BILLION PARA NEED 2.5 GB RAM GPU
SO 600 GB MODEL NEED 1500 GB RAM IMPOSIBLE FOR NORMAL PEOPLE

SO API CAME = API OF LLM HOSTED BY LARGE COMPANY WHO HAVE INFRA LIKE=
AWS -BEDROCK
AZUREAI
SAMNOVA
GROQ




-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# For Git Bash users - Complete UV setup commands:

# 1. Install UV in Git Bash       ####  curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. Add UV to PATH in Git Bash (IMPORTANT!)
echo 'export PATH="$HOME/.cargo/bin:$PATH"' >> ~/.bashrc
source ~/.bashrc

# 3. Check UV installation
uv --version

# 4. Create project
mkdir ai-project && cd ai-project

# 5. Initialize UV project
uv init

# 6. Create virtual environment
uv venv

# 7. Activate - Git Bash specific
source .venv/Scripts/activate  # Git Bash on Windows uses forward slashes

# 8. Install Gen AI packages
uv add openai langchain transformers torch chromadb

# 9. Install Jupyter for notebooks
uv add jupyter ipykernel pandas numpy matplotlib

# 10. Register kernel
uv run ipython kernel install --user --name="ai-env"

# 11. Test
uv run python -c "import torch; print(f'PyTorch: {torch.__version__}')"

# 12. Start Jupyter
uv run jupyter notebook

# Git Bash TIPS:
# - Use forward slashes (/) not backslashes (\)
# - Python scripts in .venv/Scripts/ not .venv/bin/
# - If activation fails: source .venv/Scripts/activate
# - For PATH issues: which python should show .venv path

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
# सारांश (SUMMARY) NO NEED TO READ BELOW ALL JUST 1 LINE OK      -- UV AND LIB AND FUNCTION MEANTING  ====from [LIBRARY_IN_NEUROVED] import [FUNCTION] 
--------------------------------------------------------------------------------------------------------------
# PROJECT CONTEXT: You are working in ~/OneDrive/Desktop/NEUROVED-CODES

# 1. WHERE PYTHON LOOKS
# Why: 'uv' stores your AI tools in hidden folder inside current project.  === Path: ~/OneDrive/Desktop/NEUROVED-CODES/.venv/lib/site-packages/

# 3. HOW IT WORKS IN YOUR TERMINAL
# Why: 'uv run' tells Python to look ONLY in your 'NEUROVED-CODES' environment.
uv run python main.py # Executes your file using the tools found in your local .venv.

# from [LIBRARY] import [FUNCTION]  ==== Access the (Folder) to use the (Reusable Code).

from openai import OpenAI # Go to 'NEUROVED-CODES/.venv/...' and grab the 'OpenAI' tool.
from transformers import pipeline # Go to 'NEUROVED-CODES/.venv/...' and grab the 'pipeline' tool.

# from [LIBRARY_IN_NEUROVED] import [FUNCTION] # Keeps your 'main.py' fast and clean.
# 1. LIBRARY   ==== folder containing many files of pre-written code for a specific topic.
# 2. FUNCTION (The Reusable Tool) ===specific block of code inside a library designed to perform one task.

---------------------------------------------------------------------------------------------------------------------------------------------
temperature
Temperature in machine learning controls randomness in AI responses: 0.0 = deterministic, 2.0 = highly creative. [Your Llama model uses 0.7]
Max Tokens: The hard limit set on the number of tokens (words or parts of words) the model can generate in a single response.
Context Window: The maximum amount of text (input plus output) that a model can "remember" and process at one time.
Top-P (Nucleus Sampling): A technique that limits the model’s choices to a subset of tokens whose cumulative probability reaches a specific threshold.
Top-K: A sampling method that restricts the model to choosing only from the $k$ most likely next tokens.

gpt 3.5 = 4096 tokens 
gpt5 token window = 128k tokens 
llama2 =4096 tokens 
gemini =1 million token [have largst token no.]

--------------------------------------------------------------------------------------------------------------
1 token = 4 char [char includes words , symbol@ and space]
llm creates one word which has highest probabilty at a time in output , 
llm=dictionary of vocabs consist of general info 
temprature = decides randomness of answer
- low temp -> low creativity , high temp -> high creativity 
- same answ for same question again  --> t low -> 0-0.2 -> for factual answer --> we kp low like medical 
- high temp --> movie ,poem , essay , 
- mat -- 0.9 [probability]
  gate -- 0.02
  sofa -- 0.03
temp=2
new_probability_of_mat_if i change temperature to 2 = 0.9/[0.9/2 + 0.02/2  + 0.03/2]
new_probability =[logits/temperature]*softmax
when we set t low prob gets increased 
Max tokens: Maximum number of tokens (words/characters) the AI can generate in a single response. [Your model: max_new_tokens=512]
top-p  -> p-> prob-> Top-p: Picks tokens from smallest group whose total probability reaches p (we provide 90%-95%  ).
top-k   -> Top-k: Limits sampling to exactly k most probable tokens (e.g., top 50), ignores all others.
for below example if we give k=4 thn it will select first 4 tokens only and neglect next
k=50

the       0.3
capital   0.2
of        0.1
india     0.1   
  top-p = 0.7  top-p is 0.7 then above 4 words will b given because they tootal 0.7 prob and remove low prob 
               words 
is        0.1   dis
new       0.1   dis
delhi     0.05  discards    
kolkatta  0.05  dis
raipur    0.05  dis
bhopal    0.05  dis
total     1     

SUMMARY 
temp                             top-p                    top-k
controls randomness            p=0.9-0.95                k=50 it will select top 50 words geenerated from llm vocab 
low temp -> low creativity ,    effect creativity         
high temp -> high creativity 
T = 0-0.2 FActual
    0.4-1 balanced creativity
    1<    high creat

-  we mostly used 1)temp  , 2) maxtokens to control output

langchain
framwork to create llm based apps
langchain and llamaindex same but in llama index small pay required 
-  langchain llm based app
-  langgrapph agentic ai
-  human mssage  =anything we ask to llm
-  system messag =role w assign to llm 
-  ai msg        =output msg

--------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



